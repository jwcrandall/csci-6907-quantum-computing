\documentclass[main.tex]{subfiles}
\begin{document}

\href{https://www2.seas.gwu.edu/~simhaweb/quantum/modules/review/lin-review/lin-review.html}{Linear Algebra Review}

\subsection{Relationship Between Linear Algebra and Quantum Computing}

The parts of standard linear algebra we will cover in the review will include vectors, matrices, matrix-vector multiplication, matrix-matrix multiplication, span, basis, linear independence, eigenvectors, and eigenvalues.

\subsection{Review of Basic Linear Algebra Parts I, II, III, and IV}
    
    \subsubsection{What are Vectors and what do you do with them?}
    
    \begin{enumerate}[]
        \item Scalar multiplication $$\begin{aligned} \alpha \mathbf{u} &=\alpha\left(u_{1}, \ldots, u_{n}\right) \\ &=\left(\alpha u_{1}, \ldots, \alpha u_{n}\right) \end{aligned}$$
        
        \item Vector addition $$\begin{aligned} \mathbf{u}+\mathbf{v} &=\left(u_{1}, u_{2}, \ldots, u_{n}\right)+\left(v_{1}, v_{2}, \ldots, v_{n}\right) \\ &= \left(u_{1}+v_{1}, u_{2}+v_{2}, \ldots, u_{n}+v_{n}\right) \end{aligned}$$
        
        \item Vector dot-product $$\begin{aligned} \mathbf{u} \cdot \mathbf{v} &=\left(u_{1}, u_{2}, \ldots, u_{n}\right) \cdot\left(v_{1}, v_{2}, \ldots, v_{n}\right) \\ &=u_{1} v_{1}+u_{2} v_{2}+\ldots+u_{n} v_{n} \end{aligned}$$
    \end{enumerate}
    
    Scalars changes the magnitude of the vector, addition combines vectors, and dot-products transform multiple vectors into a scalar. Suppose we have three 3D vectors 
    
    $$\begin{aligned} \mathbf{u} &=\left(u_{1}, u_{2}, u_{3}\right) \\ \mathbf{v} &=\left(v_{1}, v_{2}, v_{3}\right) \\ \mathbf{w} &=\left(w_{1}, w_{2}, w_{3}\right) \end{aligned}$$
    
    and three scalars $\alpha, \beta, \gamma$, the linear combination becomes
    
    $$\begin{aligned} \alpha \mathbf{u}+\beta \mathbf{v}+\gamma \mathbf{w} &=\alpha\left(u_{1}, u_{2}, u_{3}\right)+\beta\left(v_{1}, v_{2}, v_{3}\right)+\gamma\left(w_{1}, w_{2}, w_{3}\right) \\ &=\left(\alpha u_{1}+\beta v_{1}+\gamma w_{1}, \alpha u_{2}+\beta v_{2}+\gamma w_{2}, \alpha u_{3}+\beta v_{3}+\gamma w_{3}\right) \end{aligned}$$
    
    Whenever you "think" vector, you should think "column" style and write $\mathbf{v}=(1,2,3)$ instead of 
    
    $$\mathbf{v}=\left[\begin{array}{l}1 \\ 2 \\ 3\end{array}\right].$$
    
    Sometimes, we'll emphasize the "row" style when it's useful by explicitly transposing the column into a row vector $\mathbf{v}^{T}=\left[\begin{array}{lll}1 & 2 & 3\end{array}\right]$.
    
    \subsubsection{Dot Products, Lengths, Angles, Orthogonal Vectors}
    
    We can express the length of a vector with a dot-product:
    
    $$\begin{aligned}|\mathbf{u}| &=\text { length of } \mathbf{u} \\ &=\sqrt{\mathbf{u} \cdot \mathbf{u}} \end{aligned}$$
    
    The dot product gives us a relation with the angle between two vectors
    
    $$\mathbf{u} \cdot \mathbf{v}=|\mathbf{u}||\mathbf{v}| \cos (\theta)$$
    
    because when $\mathbf{u}=\mathbf{v}$ then 
    
    $$ \begin{aligned} \cos (\theta) &=\frac{\mathbf{u} \cdot \mathbf{v}}{|\mathbf{u}||\mathbf{v}|} \\
    &=\frac{\mathbf{u} \cdot \mathbf{u}}{|\mathbf{u}||\mathbf{u}|} \\
    &=\frac{|\mathbf{u}|^{2}}{|\mathbf{u}|^{2}} \\
    &=1
    \end{aligned}$$
    
    When $\theta=0, \cos (\theta)=1$ and is the largest possible value of the cosine. At the other extreme, the smallest value of the cosine is $0$ when the angle is $90^\circ$. Since $\cos \left(90^{\circ}\right)=0$, we can use this to define orthogonality with dot products where two vectors $\mathbf{u}$ and $\mathbf{v}$ are \textit{orthogonal} if their dot product is zero $\mathbf{u} \cdot \mathbf{v}=0$.
    
    \subsubsection{Matrix-Vector Multiplication and what it means}
    
    Think of the left side as the linear combination and the right side as the vector that results from the linear combination. Matrices came about by visually reorganizing the left side into 
    
    $$\alpha\left[\begin{array}{l}u_{1} \\ u_{2} \\ u_{3}\end{array}\right]+\beta\left[\begin{array}{l}v_{1} \\ v_{2} \\ v_{3}\end{array}\right]+\gamma\left[\begin{array}{l}v_{1} \\ v_{2} \\ v_{3}\end{array}\right]=\left[\begin{array}{lll}u_{1} & v_{1} & w_{1} \\ u_{1} & v_{2} & w_{2} \\ u_{1} & v_{3} & w_{3}\end{array}\right]\left[\begin{array}{l}\alpha \\ \beta \\ \gamma\end{array}\right]$$
    
    Since it's the same left-side (just visually) reorganized, it must equal the original right side:

    $$\left[\begin{array}{lll}u_{1} & v_{1} & w_{1} \\ u_{2} & v_{2} & w_{2} \\ u_{3} & v_{3} & w_{3}\end{array}\right]\left[\begin{array}{l}\alpha \\ \beta \\ \gamma\end{array}\right]=\left[\begin{array}{l}\alpha u_{1}+\beta v_{1}+\gamma w_{1} \\ \alpha u_{2}+\beta v_{2}+\gamma w_{2} \\ \alpha u_{3}+\beta v_{3}+\gamma w_{3}\end{array}\right]$$
    
    If we give each of these names:
    
    $$\begin{aligned} \mathbf{A} & \triangleq\left[\begin{array}{lll}u_{1} & v_{1} & w_{1} \\ u_{1} & v_{2} & w_{2} \\ u_{1} & v_{3} & w_{3}\end{array}\right] \\ \mathbf{x} & \triangleq\left[\begin{array}{l}\alpha \\ \beta \\ \gamma\end{array}\right] \\ \mathbf{b} & \triangleq\left[\begin{array}{l}\alpha u_{1}+\beta v_{1}+\gamma w_{1} \\ \alpha u_{2}+\beta v_{2}+\gamma w_{2} \\ \alpha u_{3}+\beta v_{3}+\gamma w_{3}\end{array}\right] \end{aligned}$$
    
    Then, written in these symbols, we have $\mathbf{A x}=\mathbf{b}$, of which there are two meanings. In the first one, $\mathbf{x}$ is only incidentally a vector, and contains the scalars in the linear combination. The left side of $\mathbf{A} \mathbf{x}=\mathbf{b}$ unpacks into 
    
    $$\left[\begin{array}{lll}u_{1} & v_{1} & w_{1} \\ u_{2} & v_{2} & w_{2} \\ u_{3} & v_{3} & w_{3}\end{array}\right]\left[\begin{array}{l}\alpha \\ \beta \\ \gamma\end{array}\right]=\alpha\left[\begin{array}{l}u_{1} \\ u_{2} \\ u_{3}\end{array}\right]+\beta\left[\begin{array}{l}v_{1} \\ v_{2} \\ v_{3}\end{array}\right]+\gamma\left[\begin{array}{l}v_{1} \\ v_{2} \\ v_{3}\end{array}\right]$$
    
    whereas the right side is the result vector (of the linear combination):
    
    $$\left[\begin{array}{l}\alpha u_{1}+\beta v_{1}+\gamma w_{1} \\ \alpha u_{2}+\beta v_{2}+\gamma w_{2} \\ \alpha u_{3}+\beta v_{3}+\gamma w_{3}\end{array}\right]$$
    
    When we ask whether there is an $x$ vector such that $\mathbf{A x}=\mathbf{b}$, we are asking whether there is some linear combination of the column vectors to produce the vector $\mathbf{b}$. The second interpretation is a matrix transforms a vector
    
    $$\left[\begin{array}{ccc}\cdots & \mathbf{r}_{1} & \cdots \\ \cdots & \mathbf{r}_{2} & \cdots \\ \vdots & \vdots & \vdots \\ \cdots & \mathbf{r}_{m} & \cdots\end{array}\right] \mathbf{x}=\left[\begin{array}{c}\mathbf{r}_{1} \cdot \mathbf{x} \\ \mathbf{r}_{2} \cdot \mathbf{x} \\ \vdots \\ \mathbf{r}_{m} \cdot \mathbf{x}\end{array}\right]$$
    
    \subsubsection{Solving Ax=b Exactly and Approximately}
    
    In a "vector stretch"
    
    $$x_{1}\left[\begin{array}{l}1 \\ 4\end{array}\right]+x_{2}\left[\begin{array}{l}3 \\ 2\end{array}\right]=\left[\begin{array}{r}7.5 \\ 10\end{array}\right]$$
    
    we ask is there a linear combination (values of $x_1$,$x_2$) such that the linear combination on left yields the vector on the right. In this case the solution is $x_{1}=1.5, \quad x_{2}=2$. In a case where there is no solution 
    
    $$x_{1}\left[\begin{array}{l}1 \\ 2\end{array}\right]+x_{2}\left[\begin{array}{l}3 \\ 6\end{array}\right]=\left[\begin{array}{r}7.5 \\ 10\end{array}\right]$$
    
    we notice that the vector (3,6), which is the second column is in fact a multiple of the first column (1,2). Thus, it so happens that
    
    $$3\left[\begin{array}{l}1 \\ 2\end{array}\right]-\left[\begin{array}{l}3 \\ 6\end{array}\right]=\left[\begin{array}{l}0 \\ 0\end{array}\right]$$
    
    where 
    
    $$x_{1}\left[\begin{array}{l}1 \\ 2\end{array}\right]+x_{2}\left[\begin{array}{l}3 \\ 6\end{array}\right]=\left[\begin{array}{l}0 \\ 0\end{array}\right]$$
    
    has a non-zero solution $\left(x_{1}, x_{2}\right)=(3,-1)$. This means that the two columns are not independent by the definition of linear independence, and the matrix 
    
    $$\left[\begin{array}{ll}1 & 3 \\ 2 & 6\end{array}\right]$$
    
    is not full-rank. The Reduced Row Echelon Form (RREF) where 
    
    \begin{enumerate}
        \item Nonzero rows appear above the zero rows.
        \item In any nonzero row, the first nonzero entry is a one (called the leading one).
        \item The leading one in a nonzero row appears to the left of the leading one in any lower row.
        \item If a column contains a leading one, then all the other entries in that column are zero.
    \end{enumerate}
    
    for 
    
    $$\left[\begin{array}{ll}1 & 3 \\ 2 & 6\end{array}\right]$$
    
    using elementary row operations
    
    \begin{enumerate}
        \item Divide a row by a nonzero number.
        \item Subtract a multiple of a row from another row.
        \item Interchange two rows
    \end{enumerate}
    
    (Row 2) - 2(Row 1) gives
    
    $$\left[\begin{array}{ll}1 & 3 \\ 0 & 0\end{array}\right]$$

    which contains one pivot. A pivot is any row that does not contain only zeros and its first non zero number is a 1. What do we do when no solution is possible? Consider $\mathbf{A x}=\mathbf{b}$ and $\mathbf{A}$'s columns
    
    $$\mathbf{A}=\left[\begin{array}{cccc}\vdots & \vdots & \ldots & \vdots \\ \mathbf{c}_{1} & \mathbf{c}_{2} & \ldots & \mathbf{c}_{n} \\ \vdots & \vdots & \ldots & \vdots\end{array}\right]$$
    
    In general, if $\mathbf{A} \mathbf{x}=\mathbf{b}$ does not have a solution, it means right-side vector $\mathbf{b}$ is not in the span of the columns of $\mathbf{A}$. The span of a set of vectors is all the vectors you can reach by linear combinations. Thus, there is no linear combination such that
    
    $$x_{1} \mathbf{c}_{1}+x_{2} \mathbf{c}_{2}+\ldots+x_{n} \mathbf{c}_{n}=\mathbf{b}$$
    
    Suppose we were to seek a "next best" or approximate solution $\hat{\mathbf{x}}$. The first thing to realize is that an approximate solution is still a linear combination of the columns:
    
    $$\hat{x}_{1} \mathbf{c}_{1}+\hat{x}_{2} \mathbf{c}_{2}+\ldots+\hat{x}_{n} \mathbf{c}_{n}=?$$
    
    Thus, in this example, we seek

    $$\hat{x}_{1}\left[\begin{array}{l}6 \\ 2 \\ 0\end{array}\right]+\hat{x}_{2}\left[\begin{array}{l}4 \\ 3 \\ 0\end{array}\right]+\hat{x}_{3}\left[\begin{array}{l}8 \\ 1 \\ 0\end{array}\right]$$
    
    that's closest to the desired target (5,6,3). The difference vector between the "closest" and $\mathbf{b}$ is perpendicular to every vector in the span of the columns. This is what the least-squares method exploits. Suppose $\mathbf{y}=$ the closest vector in the colspan. Let 
    
    $$\begin{aligned} \mathbf{z} &=\text { the difference vector } \\ &=\mathbf{b}-\mathbf{y} \end{aligned}$$
    
    Since $\mathbf{z}$ is perpendicular to the plane containing the columns, it's perpendicular to the columns themselves:
    
    $$\mathbf{c}_{1} \cdot \mathbf{z}=0$$
    $$\mathbf{c}_{2} \cdot \mathbf{z}=0$$
    
    The rest of least squares stems from the observation that the columns of $\mathbf{A}$ are rows of $\mathbf{A}^{T}$. If we were to multiply $\mathbf{A}^{T}$ into $\mathbf{z}$
    
    $$\mathbf{A}^{T} \mathbf{z}=\left[\begin{array}{ccc}\cdots & \mathbf{c}_{1} & \cdots \\ \cdots & \mathbf{c}_{2} & \cdots \\ \vdots & \vdots & \vdots \\ \cdots & \mathbf{c}_{n} & \cdots\end{array}\right]=\left[\begin{array}{c}\mathbf{c}_{1} \cdot \mathbf{z} \\ \mathbf{c}_{2} \cdot \mathbf{z} \\ \vdots \\ \mathbf{c}_{n} \cdot \mathbf{z}\end{array}\right]=\left[\begin{array}{l}0 \\ 0 \\ 0 \\ 0\end{array}\right]$$
    
    plug $\mathbf{y}=\mathbf{A} \hat{\mathbf{x}}$ into $\mathbf{z}=\mathbf{b}-\mathbf{y}$ and do the algebra to get 
    
    $$\mathbf{A}^{T} \mathbf{b}-\mathbf{A}^{T} \mathbf{A} \hat{\mathbf{x}}=\mathbf{0}$$
    
    after which we get
    
    $$\hat{\mathbf{x}}=\left(\mathbf{A}^{T} \mathbf{A}\right)^{-1} \mathbf{A}^{T} \mathbf{b}$$
    
    provided the inverse exists
    
    \subsubsection{Matrix-Matrix Multiplication and what it means}
    
    There are generally four contexts where we see matrix-matrix multiplication:
    
    \begin{enumerate}
        \item When the matrices represent vector-transformations and they multiply to create one matrix that does the combined transformation.
        \item When the associative rule is used to simplify terms.
        \item When a matrix's inverse multiplies into it to produce $\mathbf{I}$
        \item When one matrix transforms another matrix (which is useful in understanding row operations).
    \end{enumerate}
    
    Multiply-compatibility for matrix-matrix and matrix-vector multiplication. Two matrices $\mathbf{A}_{m \times n}$ and $\mathbf{B}_{n \times k}$ can be multiplied as $\mathbf{A B}$ to produce $\mathbf{C}_{m \times k}$. $\mathbf{A}_{m \times n} \mathbf{B}_{n \times k}=\mathbf{C}_{m \times k}$. Thus, $\mathbf{B}$ should have as many rows as $\mathbf{A}$ has columns. The same rule works for matrix-vector compatibility if we treat a column vector of dimension $n$ as having $n$ rows $\mathbf{A}_{m \times n} \mathbf{x}_{n \times 1}=\mathbf{b}_{m \times 1}$. The standard way we describe $\mathbf{B A}$ is 

    $$
    \left[\begin{array}{cccc}
    b_{11} & b_{12} & \cdots & b_{1 n} \\
    b_{21} & b_{22} & \cdots & b_{2 n} \\
    \vdots & \vdots & & \vdots \\
    b_{n 1} & b_{n 2} & \cdots & b_{n n}
    \end{array}\right]\left[\begin{array}{cccc}
    a_{11} & a_{12} & \cdots & a_{1 n} \\
    a_{21} & a_{22} & \cdots & a_{2 n} \\
    \vdots & \vdots & & \vdots \\
    a_{n 1} & a_{n 2} & \cdots & a_{n n}
    \end{array}\right]=\left[\begin{array}{cccc}
    c_{11} & c_{12} & \cdots & c_{1 n} \\
    c_{21} & c_{22} & \cdots & c_{2 n} \\
    \vdots & \vdots & & \vdots \\
    c_{n 1} & c_{n 2} & \cdots & c_{n n}
    \end{array}\right]
    $$    

    where 
    
    $$c_{i j}=\sum_{k=1}^{n} b_{i k} a_{k j}$$
    
    (this is the element in row i, column j of $\mathbf{C}$. When unpacking the matrix $\mathbf{A}$ into its columns $\mathbf{a}_{1}, \mathbf{a}_{2}, \ldots$ then this is one useful way to describe it
    
    $$
    \mathbf{B}\left[\begin{array}{cccc}
    a_{11} & a_{12} & \cdots & a_{1 n} \\
    a_{21} & a_{22} & \cdots & a_{2 n} \\
    a_{31} & a_{32} & \cdots & a_{3 n} \\
    \vdots & \vdots & \vdots & \vdots \\
    a_{n 1} & a_{n 2} & \cdots & a_{n n}
    \end{array}\right]=\mathbf{B}\left[\begin{array}{cccc} 
    & & & \\
    \vdots & \vdots & \vdots & \vdots \\
    \mathbf{a}_{1} & \mathbf{a}_{2} & \cdots & \mathbf{a}_{n} \\
    \vdots & \vdots & \vdots & \vdots
    \end{array}\right]=\left[\begin{array}{cccc} 
    & & & \\
    & & & \vdots & \vdots & \vdots \\
    \mathbf{B a}_{1} & \mathbf{B a}_{2} & \cdots & \mathbf{B a}_{n} \\
    \vdots & \vdots & \vdots & \vdots
    \end{array}\right]
    $$
    
    where the k-th column of the resulting $\mathbf{C}$ matrix is the vector $\mathbf{B a}_{k}$ that results from the matrix $\mathbf{B}$ multiplying vector $\mathbf{a}_{k}$. For the second context matrix multiplication turned out to be associative $\mathbf{A}(\mathbf{B C})=(\mathbf{A B}) \mathbf{C}$, however is not commutative. For the third context the inverse of a matrix $\mathbf{A}$, if it exists, is a matrix $\mathbf{A}^{-1}$ such that $\mathbf{A}^{-1} \mathbf{A}=\mathbf{I}$ where $\mathbf{I}$ is the identity matrix. 
    
    $$
    \left[\begin{array}{cccc}
    1 & 0 & \cdots & 0 \\
    0 & 1 & \cdots & 0 \\
    \vdots & \vdots & \vdots & \vdots \\
    0 & 0 & \cdots & 1
    \end{array}\right]
    $$
    
    Additionally, $\mathbf{A}^{-1}$ is defined only for square matrices, exists only if it's full rank, and is computed by identifying the row operations that reduce $\mathbf{A}$ to its Reduced Row Echelon Form (RREF). If RREF procedures reveal that $\mathbf{A}$ is not full-rank (fewer pivots than columns), then $\mathbf{A}^{-1}$ does not exist. If $\mathbf{A}$ is not square or $\mathbf{A^{-1}}$ does not exist we can utilize least-squares. In this case, $\mathbf{A}^{T} \mathbf{A}$ is square and likely to have an inverse. Recall the least-squares solution:
    
    $$\hat{\mathbf{x}}=\left(\mathbf{A}^{T} \mathbf{A}\right)^{-1} \mathbf{A}^{T} \mathbf{b}$$
    
    Here we need the inverse $\left(\mathbf{A}^{T} \mathbf{A}\right)^{-1}$ to exist. When there are many more rows than columns $\mathbf{A}_{m \times n}$ has a few column vectors with many dimensions. The likelihood that these are independent will be high. A somewhat difficult proof showed that if the columns of $\mathbf{A}$ are independent then $\left(\mathbf{A}^{T} \mathbf{A}\right)^{-1}$ oes indeed exist. For the fourth context matrix-multiplication as a "matrix transformer", we multiply one matrix by another and get a third $\mathbf{A B}=\mathbf{C}$, and could interpret it as $\mathbf{A}$ transforms matrix $\mathbf{B}$ into matrix $\mathbf{C}$. Such an interpretation does turn out to be useful for theoretical purposes, as we'll see when reasoning about RREFs where there are three types of row operations
    
    \begin{enumerate}
        \item Scale: divide one row by a number
        $$
        \mathbf{r}_{i} \leftarrow \frac{\mathbf{r}_{i}}{\alpha}
        $$
        \item Swap: swap two rows
        $$
        \begin{aligned}
        \operatorname{temp} & \leftarrow \mathbf{r}_{i} \\
        \mathbf{r}_{i} & \leftarrow \mathbf{r}_{j} \\
        \mathbf{r}_{j} & \leftarrow \text { temp }
        \end{aligned}
        $$    
        \item Replace a row by adding a multiple of another row.
        $$
        \mathbf{r}_{i} \leftarrow \mathbf{r}_{i}+\alpha \mathbf{r}_{j}
        $$
    \end{enumerate}
    
    There is also an order in which we change coefficients, always start with row 1, column 1, trying to make that element a pivot, when we're successful making an element into a pivot, we change coefficients below the pivot to zero. The search for the next pivot moves to the next row, next column. A search may not be successful, in which case we move to the next column (same row). fter the final pivot we go back to the first one and start the process of creating zeroes above each pivot. A full-rank RREF (a pivot in every column), tells us the equations have a unique solution. When we get an RREF at the end of $k$ row operations (transformations) we can describe it as $\operatorname{RREF}(\mathbf{A})=\mathbf{R}_{k} \mathbf{R}_{k-1} \cdots \mathbf{R}_{2} \mathbf{R}_{1} \mathbf{A}$ where the first row op is acheived by $\mathbf{R}_{1}$ the second is represented by $\mathbf{R}_{2}$ applied to the result, and so on. So, the question is: what do we get when applying $\mathbf{R}_{k} \mathbf{R}_{k-1} \cdots \mathbf{R}_{2} \mathbf{R}_{1} \mathbf{I}=?$ \\
    
    Start with $\operatorname{RREF}(\mathbf{A})=\mathbf{R}_{k} \mathbf{R}_{k-1} \ldots \mathbf{R}_{2} \mathbf{R}_{1} \mathbf{A}$ and recognize that $\operatorname{RREF}(\mathbf{A})=\mathbf{I}$, and multiply all the $\mathbf{R}_{i}$ matrices into one matrix $\mathbf{R} \triangleq \mathbf{R}_{k} \mathbf{R}_{k-1} \ldots \mathbf{R}_{2} \mathbf{R}_{1}$. The reduction to RREF can be written as $\mathbf{I}=\mathbf{R} \mathbf{A}$ which means that $\mathbf{R}$ is the inverse of $\mathbf{A}$.\\
    
    If we've already solved for the variables, why do we need the inverse? We started with wanting to solve $\mathbf{A x}=\mathbf{b}$ for some $\mathbf{A}$ and some $\mathbf{b}$, the given equations. We apply row reductions to get $\operatorname{RREF}(\mathbf{A})$. That gave us the solution $\mathbf{x}$. Isn't that enough? Why also compute $\mathbf{A}^{-1}$ on the right side? In applications, it turns out that very often the equations stay the same but the right side changes to $\mathbf{A} \mathbf{x}=\mathbf{c}$. We solve once to get $\mathbf{A}^{-1}$ and apply it to a new right side $\mathbf{X}=\mathbf{A}^{-1} \mathbf{c}$. In other applications like least-squares, we need to compute the inverse of matrices like $\left(\mathbf{A}^{T} \mathbf{A}\right)$.
    
    \subsubsection{Spaces, Span, Independence, Basis}
    
    The span is all the vectors you can get by linear combinations $\operatorname{span}(\mathbf{u}, \mathbf{v})=\{\mathbf{z}: \mathbf{z}=\alpha \mathbf{u}+\beta \mathbf{v}$, for some $\alpha, \beta\}$ read as "all z such that z can be expressed as a linear combination of u and v". A plane is "complete" if that it contains all linear combinations of any subset of vectors. Such a collection is called a subspace. A subspace is a collection of vectors where the linear combination of any two vectors is in the collection. A basis for a subspace is any minimal collection of vectors whose span is the subspace. Two vectors are not enough to span a 3D space. A basis for a 3D space needs three, and three that are not co-planar. We are often interested in an orthogonal basis. Consider $\mathbf{u}=(4,0,2)$ and $\left.\mathbf{r}=\left(3,-\frac{25}{2},-6\right)\right)$, $\mathbf{u} \cdot \mathbf{r}=(4,0,2) \cdot\left(3,-\frac{25}{2},-6\right)=0$. So, these two are orthogonal and because two vectors are enough for a 2D plane, they form an orthogonal basis.\\
    
    For orthogonal spaces, consider the vectors $\mathbf{y}$ and $\mathbf{w}$ both of which are on the same line. $\operatorname{span}(\mathbf{y}, \mathbf{w})$ is a subspace, because all linear combinations of any two vectors on this line are on the line. Consider all the vectors orthogonal to $\mathbf{y}$. These are all going to be on a plane to which the line is perpendicular. Let $\mathbf{S}$ be the subspace of vectors on this perpendicular plane. Every vector in $\mathbf{S}$ is perpendicular to every vector $\operatorname{span}(\mathbf{y}, \mathbf{w})$. Thus, $\mathbf{S}$ and span $(\mathbf{y}, \mathbf{w})$ are orthogonal subspaces $\mathbf{S}^{\perp}=\operatorname{span}(\mathbf{y}, \mathbf{w})$ and $\mathbf{S}=\operatorname{span}(\mathbf{y}, \mathbf{w})^{\perp}$.\\
    
    Vectors $\mathbf{v}_{1}, \mathbf{v}_{2}, \ldots, \mathbf{v}_{n}$ are linearly independent if the only solution to the equation $x_{1} \mathbf{v}_{1}+x_{2} \mathbf{v}_{2}+\ldots+x_{n} \mathbf{v}_{n}=\mathbf{0}$ is $x_{1}=x_{2}=\ldots=x_{n}=0$. Given a collection of vectors, we can check that they are independent by placing the vectors as columns of a matrix $\mathbf{A}$:
    
    $$
    \mathbf{A}=\left[\begin{array}{cccc} 
    & & & \\
    \vdots & \vdots & \ldots & \vdots \\
    \mathbf{v}_{1} & \mathbf{v}_{2} & \ldots & \mathbf{v}_{n} \\
    \vdots & \vdots & \ldots & \vdots
    & & & \\
    \end{array}\right]
    $$
    
    so $x_{1} \mathbf{v}_{1}+x_{2} \mathbf{v}_{2}+\ldots+x_{n} \mathbf{v}_{n}=\mathbf{0}$ becomes 

    $$
    \left[\begin{array}{cccc} 
    & & & \\
    \vdots & \vdots & \ldots & \vdots \\
    \mathbf{v}_{1} & \mathbf{v}_{2} & \ldots & \mathbf{v}_{n} \\
    \vdots & \vdots & \ldots & \vdots
    \end{array}\right]\left[\begin{array}{c}
    x_{1} \\
    x_{2} \\
    x_{3} \\
    \vdots \\
    x_{n}
    \end{array}\right]=\left[\begin{array}{c}
    0 \\
    0 \\
    0 \\
    \vdots \\
    0
    \end{array}\right]
    $$
    
    or, more compactly, solve $\mathbf{A x}=\mathbf{0}$. When RREF(A) = I (i.e., the RREF is full-rank), then $\mathbf{x}=\mathbf{0}$ is the only solution to $\mathbf{A x}=\mathbf{0}$. Thus, the vectors will be independent if the RREF is full-rank (a pivot in every column). This gives us a means to identify whether a collection of vectors is independent: put them in a matrix and check its RREF.\\
    
    To review the rowspace and colspace of a matrix consider the matrix
    
    $$
    \mathbf{A}=\left[\begin{array}{ccccc}
    1 & 1 & 1 & 0 & 3 \\
    -1 & 0 & 1 & 1 & -1 \\
    0 & 1 & 2 & 1 & 2 \\
    0 & 0 & 0 & 0 & 0
    \end{array}\right]
    $$
    
    treat the rows as vectors and name them
    
    $$
    \begin{aligned}
    &\mathbf{r}_{1}=(1,1,1,0,3) \\
    &\mathbf{r}_{2}=(-1,0,1,1,-1) \\
    &\mathbf{r}_{3}=(0,1,2,1,2) \\
    &\mathbf{r}_{4}=(0,0,0,0,0)
    \end{aligned}
    $$
    
    The span of these vectors is the rowspace of the matrix $\operatorname{rowspace}(\mathbf{A})=\operatorname{span}\left(\mathbf{r}_{1}, \mathbf{r}_{2}, \mathbf{r}_{3}, \mathbf{r}_{4}\right)$. That is, any linear combination of the four row vectors is in the rowspace. For example 
    
    $$
    \begin{aligned}
    2 \mathbf{r}_{1}+3 \mathbf{r}_{2}+0 \mathbf{r}_{3}+5 \mathbf{r}_{4} &=2(1,1,1,0,3)+3(-1,0,1,1,-1)+0(0,1,2,1,2)+5(0,0,0,0,0) \\
    &=(-1,2,5,3,3)
    \end{aligned}
    $$
    
    So (−1,2,5,3,3) is a vector in the rowspace. Similarly, if we name the 5 columns $\mathbf{c}_{1}, \mathbf{c}_{2}, \mathbf{c}_{3}, \mathbf{c}_{4}, \mathbf{c}_{5}$ then the colspace is their span $\operatorname{colspace}(\mathbf{A})=\operatorname{span}\left(\mathbf{c}_{1}, \mathbf{c}_{2}, \mathbf{c}_{3}, \mathbf{c}_{4}, \mathbf{c}_{5}\right)$. When checking the dimension of the rowspace the RREF turns out to be
    
    $$
    \left[\begin{array}{ccccc}
    \mathbf{1} & 0 & -1 & -1 & 1 \\
    0 & \mathbf{1} & 2 & 1 & 2 \\
    0 & 0 & 0 & 0 & 0 \\
    0 & 0 & 0 & 0 & 0
    \end{array}\right]
    $$
    
    where the rowspace of the RREF is the span of the first two rows. The key observation is that the row operations we performed do not change the rowspace of the matrices along the way from $\mathbf{A}$ to its RREF. This is because all row operations are linear combinations of rows. However, it is not true that $\operatorname{colspace}(\mathbf{A})=\operatorname{colspace}(R R E F(\mathbf{A}))$ What is true is that the size of the basis for $\operatorname{colspace}(\mathbf{A})$ is the same as the number of pivot columns. Because pivots are alone in both their rows and columns, we get the result that the dimension of the rowspace is the same as the dimension of the colspace.

    \subsubsection{Orthogonality}
    
    There are two subtopics of orthogonality that commonly appear in applications, 1. Orthogonal vectors and matrices, and 2. Projections. Two vectors $\mathbf{u}$ and $\mathbf{v}$ are orthogonal if $\mathbf{u} \cdot \mathbf{v}=0$. The definition comes from the angle between them, which is a right-angle: $\cos (\theta)=\frac{\mathbf{u} \cdot \mathbf{v}}{|\mathbf{u}||\mathbf{v}|}$. Since the lengths aren't zero, if the dot product is zero, that implies a zero cosine, or $\theta=90^{\circ}$. This definition extends to a collection of vectors: Consider vectors $\mathbf{v}_{1}, \mathbf{v}_{2}, \mathbf{v}_{3}$. The collection is orthogonal if 
    
    $$
    \begin{aligned}
    &\mathbf{v}_{1} \cdot \mathbf{v}_{2}=0 \\
    &\mathbf{v}_{1} \cdot \mathbf{v}_{3}=0 \\
    &\mathbf{v}_{2} \cdot \mathbf{v}_{3}=0
    \end{aligned}
    $$
    
    Since dot-product is commutative we don't need to specify products like $\mathbf{v}_{2} \cdot \mathbf{v}_{2}=0$. Now consider the equation $\alpha_{1} \mathbf{v}_{1}+\alpha_{2} \mathbf{v}_{2}+\alpha_{3} \mathbf{v}_{3}=\mathbf{0}$. Does this imply that all the $\alpha$'s are zero, and therefore the $\mathbf{v}$'s are independent? Multiply (dot product) both sides by $\mathbf{v}_1$: $\mathbf{v}_{1} \cdot\left(\alpha_{1} \mathbf{v}_{1}+\alpha_{2} \mathbf{v}_{2}+\alpha_{3} \mathbf{v}_{3}\right)=\mathbf{v}_{1} \cdot \mathbf{0}$. The right side becomes the number 0. Do the algebra on the left side, passing the dot into the parenthesis: $\alpha_{1}\left(\mathbf{v}_{1} \cdot \mathbf{v}_{1}\right)+\alpha_{2}\left(\mathbf{v}_{1} \cdot \mathbf{v}_{2}\right)+\alpha_{3}\left(\mathbf{v}_{1} \cdot \mathbf{v}_{3}\right)=0$. Because they are orthogonal, the only non-zero dot product is the first one $\alpha_{1}\left(\mathbf{v}_{1} \cdot \mathbf{v}_{1}\right)+0+0=0$. Now apply the dot product $\mathbf{v}_{1} \cdot \mathbf{v}_{1}=\left|\mathbf{v}_{1}\right|\left|\mathbf{v}_{1}\right| \cos (0)$ and we get $\alpha_{1}\left|\mathbf{v}_{1}\right|\left|\mathbf{v}_{1}\right|=0$ which, because $\left|\mathbf{v}_{1}\right| \neq 0$, implies that, $\alpha_{1}=0$. To conclude, an orthogonal collection is linearly independent.\\
    
    What about complex vectors? Consider these two 3-component complex vectors $\mathbf{u}=(1+0 i,-0.5+0.866 i,-0.5-0.866 i)$, and $\mathbf{v}=(1+0 i, 1+0 i, 1+0 i)$. Their dot-product is $(1+0 i)(1-0 i)+(-0.5+0.866 i)(1-0 i)+(-0.5-0.866 i)(1-0 i)=0$. Thus, we define orthogonality between two complex vectors to mean: their dot-product is zero.\\
    
    To differentiate between orthogonal vs orthonormal consider two vectors $\mathbf{u}$ and $\mathbf{v}$ are orthonormal if 1. They are orthogonal, and 2. They have unit length $|\mathbf{u}|=1$, $|\mathbf{v}|=1$. An orthogonal matrix is a matrix whose columns are pairwise (all possible pairs) orthonormal. For example, consider 
    
    $$
    \mathbf{A}=\left[\begin{array}{ccc}
    4 & 4 & 6 \\
    4 & -8 & 0 \\
    4 & 4 & -6
    \end{array}\right]
    $$
    
    You can check that the columns are pairwise orthogonal but not orthonormal. For example: $|(4,4,4)|=\sqrt{4^{2}+4^{2}+4^{2}}=6.92820$. However, this matrix is orthonormal: 
    
    $$
    \mathbf{Q}=\left[\begin{array}{ccc}
    0.577 & 0.408 & 0.707 \\
    0.577 & -0.816 & 0.408 \\
    0.577 & 0 & -0.707
    \end{array}\right]
    $$
    
    The columns are pairwise orthogonal and all have unit length. What's nice about an orthogonal matrix is this: $\mathbf{Q}^{T} \mathbf{Q}=\mathbf{I}$. With the above example: 
    
    $$
    \mathbf{Q}^{T} \mathbf{Q}=\left[\begin{array}{ccc}
    0.577 & 0.577 & 0.577 \\
    0.408 & -0.816 & 0 \\
    0.707 & 0.408 & -0.707
    \end{array}\right]\left[\begin{array}{ccc}
    0.577 & 0.408 & 0.707 \\
    0.577 & -0.816 & 0.408 \\
    0.577 & 0 & -0.707
    \end{array}\right]=\left[\begin{array}{lll}
    1 & 0 & 0 \\
    0 & 1 & 0 \\
    0 & 0 & 1
    \end{array}\right]
    $$
    
    The row $i$, column $j$ entry of the product is the dot product of row i from $\mathbf{Q}^{T}$ and column $j$ from $\mathbf{Q}$. But row i from $\mathbf{Q}^{T}$ is just column i from $\mathbf{Q}$. When $i \neq j$ column i and column j from $\mathbf{Q}$ are orthonormal and so their dot-product is 0. When $\mathrm{i}=\mathrm{j}$, it's the dot-product of a column with itself, which is $1$. The important implication is that $\mathbf{Q}^{T}$ is the inverse of $\mathbf{Q}$.
    
    \subsubsection{Projections}
    
    The projection of vector $\mathbf{w}$ on vector $\mathbf{v}$ is that vector $\mathbf{y}$ along $\mathbf{v}$ which will make the difference perpendicular (dot product zero): $(\mathbf{w}-\alpha \mathbf{v}) \cdot \mathbf{v}=0$. That is, there is some stretch $\alpha \mathbf{v}$ of $\mathbf{v}$ which will make the difference $\mathbf{z}$ perpendicular to $\mathbf{v}$. We can solve for the number $\alpha=\frac{\mathbf{w} \cdot \mathbf{v}}{\mathbf{v} \cdot \mathbf{v}}$. \\
    
    Let's look at a 3D vector that's projected onto a plane whose basis is orthogonal. Think of $\mathbf{w}$ as a regular $3 \mathrm{D}$ vector sticking out into $3 \mathrm{D}$ space. Next, let $\mathbf{v}_{1}$ and $\mathbf{v}_{2}$ be two orthogonal vectors in the x-y plane. We want to ask: what's the projection of $\mathbf{w}$ onto each of $\mathbf{v}_{1}$ and $\mathbf{v}_{2}$ and what do those individual projections have to do with $\mathbf{y}$, the projection of $\mathbf{w}$ on the span of the two vectors? The individual projections are: $\alpha_{1} \mathbf{v}_{1}$, and $\alpha_{2} \mathbf{v}_{2}$ where $\alpha_{1}=\frac{\mathbf{w} \cdot \mathbf{v}_{1}}{\mathbf{v}_{1} \cdot \mathbf{v}_{1}}$ and $\alpha_{2}=\frac{\mathbf{w} \cdot \mathbf{v}_{2}}{\mathbf{v}_{2} \cdot \mathbf{v}_{2}}$. But the sum of these is exactly $\mathbf{y}=\alpha_{1} \mathbf{v}_{1}+\alpha_{2} \mathbf{v}_{2}$. Note: you cannot reconstruct the original vector $\mathbf{w}$ knowing only the individual projections $\alpha_{1} \mathbf{v}_{1}$ and $\alpha_{2} \mathbf{v}_{2}$. That's because $\mathbf{w}$ is not in the same space as $\mathbf{v}_{1}$ and $\mathbf{v}_{2}$.\\
    
    In the case when $\mathbf{w}$ is in the span of $\mathbf{v}_{1}$ and $\mathbf{v}_{2}$, the individual projections add up to the original vector $\mathbf{w}=\alpha_{1} \mathbf{v}_{1}+\alpha_{2} \mathbf{v}_{2}$. Thus, when $\mathbf{w}$ is in the span of $\mathbf{v}_{1}$ and $\mathbf{v}_{2}$, we can fully reconstruct $\mathbf{w}$ knowing only the individual projections on the basis vectors. Note: although we have used a basis for the x-y plane, the above reasoning applies to any subspace and any orthogonal basis for that subspace. Thus, in the general case we'd say that $\mathbf{w}=\alpha_{1} \mathbf{v}_{1}+\alpha_{2} \mathbf{v}_{2} \ldots+\alpha_{n} \mathbf{v}_{n}$ where $\alpha_{i}=\frac{\mathbf{w} \cdot \mathbf{v}_{i}}{\mathbf{v}_{i} \cdot \mathbf{v}_{i}}$. When the $\mathbf{v}_{i}$ 's are orthonormal, they have unit length and so $\mathbf{v}_{i} \cdot \mathbf{v}_{i}=\left|\mathbf{v}_{i}\right|^{2}=1$. Which means $\alpha_{i}=\mathbf{w} \cdot \mathbf{v}_{i}$ and then $\mathbf{w}=\left(\mathbf{w} \cdot \mathbf{v}_{1}\right) \mathbf{v}_{1}+\ldots+\left(\mathbf{w} \cdot \mathbf{v}_{n}\right) \mathbf{v}_{n}$.
    
    \subsubsection{Summary of Useful Results}
    
    \begin{itemize}
        \item Properties of matrix multiplication:

        $$
        \begin{aligned}
        \mathbf{A B} & \neq \mathbf{B A} & \text{Typically not commutative}\\
        \mathbf{A}(\mathbf{B C}) &=(\mathbf{A B}) \mathbf{C} & \text{Always associative}\\
        \mathbf{A}(\mathbf{B}+\mathbf{C}) &=(\mathbf{A B})+(\mathbf{A C}) & \text{Distributes over addition}\\
        (\mathbf{A}+\mathbf{B}) \mathbf{C} &=(\mathbf{A} \mathbf{C})+(\mathbf{B C}) & \text{Distributes over addition}\\
        \alpha(\mathbf{A B}) &=(\alpha \mathbf{A}) \mathbf{B}=\mathbf{A}(\alpha \mathbf{B}) & \text{Scalars move freely} 
        \end{aligned}
        $$

        \item A different way to view matrix-matrix multiplication:
        
        $$
        \mathbf{A}\left[\begin{array}{cccc} 
        & & & \\
        \vdots & \vdots & \vdots & \vdots \\
        \mathbf{b}_{1} & \mathbf{b}_{2} & \cdots & \mathbf{b}_{n} \\
        \vdots & \vdots & \vdots & \vdots
        & & & \\
        \end{array}\right]=\left[\begin{array}{cccc}
        & & & \\
        \vdots & \vdots & \vdots & \vdots \\
        \mathbf{A b}_{1} & \mathbf{A b}_{2} & \cdots & \mathbf{A} \mathbf{b}_{n} \\
        \vdots & \vdots & \vdots & \vdots \\
        & & &
        \end{array}\right]
        $$
        
        \item Reverse order rules: 
        
        $$
        \begin{aligned}
        (\mathbf{A B})^{T} & =\mathbf{B}^{T} \mathbf{A}^{T} & \text{Transpose of a product} \\
        (\mathbf{A B})^{-1} & =\mathbf{B}^{-1} \mathbf{A}^{-1} & \text{Inverse of a product}
        \end{aligned}
        $$
        
        \item Relationship between dot-product and angle between two vectors:
        
        $$
        \cos (\theta)=\frac{\mathbf{v} \cdot \mathbf{u}}{|\mathbf{v}||\mathbf{u}|}
        $$
        
        \item If $\mathbf{A}$ has orthonormal columns, then $\mathbf{A}^{T} \mathbf{A}=\mathbf{I}$ which makes  $\mathbf{A}^{-1}=\mathbf{A}^{T}$.
        
        \item If $\mathbf{v}_{1}, \mathbf{v}_{2}, \ldots, \mathbf{v}_{n}$ is a collection of vectors and $\mathbf{A}$ is a matrix that has these vectors as columns, then if columns $i_{1}, i_{2}, \ldots, i_{k}$ are the pivot columns of $R R E F(\mathbf{A})$, then $\mathbf{v}_{i_{1}}, \mathbf{v}_{i_{2}}, \ldots, \mathbf{v}_{i_{k}}$ are independent vectors.
        
        \item The rowspace dimension (minimum number of vectors needed to span the rowspace) is the same as the colspace dimension: $\operatorname{dim}(\text{rowspace}(\mathbf{A}))=\operatorname{dim}(\operatorname{colspace}(\mathbf{A}))$
        
        \item A collection of mutually-orthogonal vectors is linearly independent.
        
        \item If the columns of $\mathbf{A}_{m \times n}$ are linearly independent, then $\left(\mathbf{A}^{T} \mathbf{A}\right)^{-1}$ exists. This is useful in the least squares solution $\hat{\mathbf{x}}=\left(\mathbf{A}^{T} \mathbf{A}\right)^{-1} \mathbf{A}^{T} \mathbf{b}$. In most applications, we'll typically have many more rows than columns, and so it's very likely that the columns are independent. Also, if the system $\mathbf{A x}=\mathbf{b}$ has a solution, the least squares solution is an exact solution.
        
        \item The Gram-Schmidt algorithm takes a collection of linearly independent vectors and produces an orthogonal (orthonormal, if tweaked) basis for the span of those vectors. Another way of saying it: it turns a non-orthogonal basis into an orthogonal one. 
        
        \item The following are equivalent (any one implies the other two):
        
        \begin{enumerate}
            \item $\mathbf{Q}$ has orthonormal columns.
            \item $|\mathbf{Q} \mathbf{x}|=|\mathbf{x}|$, Transformation by $\mathbf{Q}$ preserves lengths.
            \item $(\mathbf{Q x}) \cdot(\mathbf{Q y})=\mathbf{x} \cdot \mathbf{y}$, Transformation by $\mathbf{Q}$ preserves dot-products.
        \end{enumerate}
    
    \end{itemize}       
    
    \subsubsection{Summary of Some Key Theorems}
    
    \begin{itemize}
    
        \item The main result that ties together inverses and equation solutions for square matrices: The following are equivalent (each implies any of the others) for a real matrix $\mathbf{A}_{n \times n}$.
        
        \begin{enumerate}
            \item $\mathbf{A}$ is invertible (the inverse exists).
            \item $\mathbf{A}^{T}$ is invertible.
            \item $RREF(\mathbf{A})=\mathbf{I}$
            \item $\operatorname{rank}(A)=n$.
            \item The rows are linearly independent.
            \item The columns are linearly independent.
            \item $\operatorname{nullspace}(\mathbf{A})=\{\mathbf{0}\}$ $\triangleright \mathbf{A} \mathbf{x}=\mathbf{0}$ has $\mathbf{x}=\mathbf{0}$ as the only solution.
            \item $\mathbf{A x}=\mathbf{b}$ has a unique solution.
            \item $\operatorname{colspace}(\mathbf{A})=\operatorname{rowspace}(\mathbf{A})=\mathbb{R}^{n}$
        \end{enumerate}
        
        \item For any matrix $\mathbf{A}_{m \times n}$, $\operatorname{rank}(\mathbf{A})=\operatorname{rank}\left(\mathbf{A}^{T} \mathbf{A}\right)$
        
        \item The singular value decomposition: any real matrix $\mathbf{A}_{m \times n}$ with rank r can be written as the product $\mathbf{A}=\mathbf{U} \boldsymbol{\Sigma} \mathbf{V}^{T}$ where $\mathbf{U}_{m \times m}$ and $\mathbf{V}_{n \times n}$ are real orthogonal matrices and $\boldsymbol{\Sigma}_{m \times n}$ is a real diagonal matrix of the form
        
            $$
            \boldsymbol{\Sigma}=\left[\begin{array}{ccccccc}
            \sigma_{1} & 0 & 0 & \ldots & & & 0 \\
            0 & \sigma_{2} & 0 & \ldots & & & 0 \\
            \vdots & \vdots & \ddots & & & & 0 \\
            0 & 0 & \ldots & \sigma_{r} & & \ldots & 0 \\
            0 & \ldots & & & 0 & \ldots & 0 \\
            0 & \ldots & & & \ldots & \ddots & 0 \\
            0 & \ldots & & & & \ldots & 0
            \end{array}\right]
            $$
        
        \item For every linear transformation $T$ there is an equivalent matrix $\mathbf{A}$ such that $T(\mathbf{x})=\mathbf{A} \mathbf{x}$ for all $\mathbf{x}$. Think of a transformation $T(\mathbf{x})$ as something that does something to a vector. An example: square the components of a vector: $T\left(x_{1}, x_{2}, \ldots, x_{n}\right)=\left(\left(x_{1}^{2}, x_{2}^{2}, \ldots, x_{n}^{2}\right)\right$. Thus, a transformation produces another vector. A linear transformation satisfies the property: $T(\alpha \mathbf{x}+\beta \mathbf{y})=\alpha T(\mathbf{x})+\beta T(\mathbf{y})$ for all scalars $\alpha, \beta$ and all vectors $\mathbf{x}$. (The squaring example above is not linear.) Let's now expand $\mathbf{x}$ in terms of a basis like the standard basis and apply the linearity of $T$ :
        $$
        \begin{aligned}
        T(\mathbf{x}) &=T\left(x_{1} \mathbf{e}_{1}+x_{2} \mathbf{e}_{2}+\ldots+x_{n} \mathbf{e}_{n}\right) \\
        &=x_{1} T\left(\mathbf{e}_{1}\right)+x_{2} T\left(\mathbf{e}_{2}\right)+\ldots+x_{n} T\left(\mathbf{e}_{n}\right)
        \end{aligned}
        $$
        What this means: once a linear transformation has taken a linear combination of the standard vectors, it is forced to produce the same linear combination of the transformed standard vectors. Thus, a linear transformations actions on standard vectors completely determine its action on any vector. This becomes the equivalent matrix (with the $T\left(\mathbf{e}_{i}\right)$'s as columns).
        
        \item Why should we care about linear transformations when we could instead work with their matrix representations? The linear transformation version is useful in proofs. Linear transformations generalize beyond real vectors to functions.
        
        \item Consider $\mathbb{R}^{n}$, the set of all $\mathrm{n}$-component vectors: Then, $\mathbb{R}^{n}$ needs exactly $n$ vectors for a basis. That is, $n$ linearly independent vectors are sufficient and necessary.
        
        \item If $\mathbf{w}$ is in the span of linearly independent vectors $\mathbf{v}_{1}, \mathbf{v}_{2}, \ldots, \mathbf{v}_{m}$, then there's a unique linear combination that expresses $\mathbf{w}$ in terms of the $\mathbf{v}_{i}$'s. To prove this, try two different linear combinations and subtract. Then use the linear independence of the $\mathbf{v}_{i}$'s.
    
    \end{itemize}
    
    \subsubsection{Change of Basis for Vectors}
    
    \subsubsection{Change of Basis for Matrices}
    
    \subsubsection{What Basis Should a (Transform) Matrix Use?}

\subsection{Some Useful Facts}

\subsection{Inner-Products and Outer-Products}

\subsection{Eigenvectors and Eigenvalues: A Review}

\subsection{Unit Length Vectors and Their Importance}

\subsection{Projectors, Completeness}

\end{document}